{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39eb6733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:03:30.864190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750140211.657460    5514 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750140212.781888    5514 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750140214.393089    5514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750140214.393157    5514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750140214.393163    5514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750140214.393168    5514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-17 09:03:34.745687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (10000, 784)\n",
      "Training labels shape: (10000, 10)\n",
      "Epoch 1/10, Loss: 0.0087, Accuracy: 0.7737\n",
      "Epoch 2/10, Loss: 0.0058, Accuracy: 0.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5514/2463515840.py:94: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/tmp/ipykernel_5514/2463515840.py:98: RuntimeWarning: overflow encountered in multiply\n",
      "  return x * (1 - x)\n",
      "/tmp/ipykernel_5514/2463515840.py:167: RuntimeWarning: overflow encountered in multiply\n",
      "  weight_update = np.outer(errors[l+1] * self.sigmoid_derivative(states[l+1]), states[l])\n",
      "/home/hope/Project package/Predictive-Coding-Models/venv/lib/python3.12/site-packages/numpy/_core/numeric.py:983: RuntimeWarning: invalid value encountered in multiply\n",
      "  return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: nan, Accuracy: 0.5520\n",
      "Epoch 4/10, Loss: nan, Accuracy: 0.1001\n",
      "Epoch 5/10, Loss: nan, Accuracy: 0.1001\n",
      "Epoch 6/10, Loss: nan, Accuracy: 0.1001\n",
      "Epoch 7/10, Loss: nan, Accuracy: 0.1001\n",
      "Epoch 8/10, Loss: nan, Accuracy: 0.1001\n",
      "Epoch 9/10, Loss: nan, Accuracy: 0.1001\n",
      "Epoch 10/10, Loss: nan, Accuracy: 0.1001\n",
      "Test Accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n## Conclusion\\n\\nThis implementation demonstrates a basic predictive coding model for MNIST classification. With more tuning (more iterations, larger network, more training data), the accuracy could be improved further. Predictive coding offers a biologically plausible alternative to backpropagation that performs local, iterative updates rather than global error propagation.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# Predictive Coding Model for MNIST Classification\n",
    "\n",
    "This notebook implements a 3-layer predictive coding model for classifying handwritten digits from the MNIST dataset. Predictive coding is a neuroscience-inspired algorithm where each layer tries to predict the activity of the layer below, and errors in these predictions drive learning.\n",
    "\n",
    "## Model Architecture\n",
    "- Layer 1 (Input layer): 784 units (28x28 pixels)\n",
    "- Layer 2 (Hidden layer): 256 units\n",
    "- Layer 3 (Output layer): 10 units (one per digit class)\n",
    "\n",
    "At each layer, we'll:\n",
    "1. Make predictions\n",
    "2. Compute prediction errors\n",
    "3. Update states (representations)\n",
    "4. Update weights\n",
    "\"\"\"\n",
    "# %%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Data Preparation\n",
    "First, let's load and preprocess the MNIST dataset.\n",
    "\"\"\"\n",
    "# %%\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(X, y, n_samples=None):\n",
    "    if n_samples is not None:\n",
    "        X = X[:n_samples]\n",
    "        y = y[:n_samples]\n",
    "    \n",
    "    # Flatten images and normalize\n",
    "    X_flat = X.reshape(X.shape[0], -1).astype('float32')\n",
    "    X_norm = MinMaxScaler().fit_transform(X_flat)\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    y_onehot = np.zeros((len(y), 10))\n",
    "    y_onehot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return X_norm, y_onehot\n",
    "\n",
    "# Use smaller subset for faster training (can increase later)\n",
    "X_train_prep, y_train_prep = preprocess_data(X_train, y_train, n_samples=10000)\n",
    "X_test_prep, y_test_prep = preprocess_data(X_test, y_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train_prep.shape}\")\n",
    "print(f\"Training labels shape: {y_train_prep.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Predictive Coding Model Implementation\n",
    "\n",
    "The predictive coding model works as follows:\n",
    "1. **Forward pass**: Each layer predicts the activity of the layer below\n",
    "2. **Error calculation**: Compare predictions with actual activity\n",
    "3. **Backward pass**: Update states and weights based on prediction errors\n",
    "\"\"\"\n",
    "# %%\n",
    "class PredictiveCodingModel:\n",
    "    def __init__(self, layer_sizes=[784, 256, 10], learning_rate=0.001, n_iterations=10):\n",
    "        \"\"\"\n",
    "        Initialize the predictive coding model.\n",
    "        \n",
    "        Parameters:\n",
    "        - layer_sizes: List of layer sizes (input, hidden, output)\n",
    "        - learning_rate: Learning rate for weight updates\n",
    "        - n_iterations: Number of inference iterations per sample\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.weights = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i+1]))\n",
    "            self.weights.append(np.random.uniform(-limit, limit, (layer_sizes[i+1], layer_sizes[i])))\n",
    "        \n",
    "        # Initialize lateral connection weights (for within-layer interactions)\n",
    "        self.lateral_weights = []\n",
    "        for size in layer_sizes[1:]:  # No lateral connections for input layer\n",
    "            self.lateral_weights.append(np.zeros((size, size)))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "        Returns predictions at each layer.\n",
    "        \"\"\"\n",
    "        layer_predictions = []\n",
    "        current_activity = x\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            # Predict next layer's activity\n",
    "            prediction = self.sigmoid(np.dot(self.weights[i], current_activity))\n",
    "            layer_predictions.append(prediction)\n",
    "            current_activity = prediction\n",
    "        \n",
    "        return layer_predictions\n",
    "    \n",
    "    def update_states_and_weights(self, x, y):\n",
    "        \"\"\"\n",
    "        Perform predictive coding inference and learning for one sample.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input sample\n",
    "        - y: Target output (one-hot encoded)\n",
    "        \n",
    "        Returns:\n",
    "        - Final predictions at each layer\n",
    "        - Errors at each layer\n",
    "        \"\"\"\n",
    "        # Initialize layer states (start with forward pass)\n",
    "        states = [x.copy()]\n",
    "        predictions = self.forward_pass(x)\n",
    "        states.extend(predictions)\n",
    "        \n",
    "        # Replace last layer's prediction with target (for supervised learning)\n",
    "        states[-1] = y.copy()\n",
    "        \n",
    "        # Store errors at each layer\n",
    "        errors = [np.zeros_like(s) for s in states]\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            # Update states and compute errors from top to bottom\n",
    "            for l in range(len(self.weights), 0, -1):\n",
    "                # Compute prediction error (bottom-up)\n",
    "                if l == len(self.weights):  # Top layer\n",
    "                    errors[l] = states[l] - predictions[l-1]\n",
    "                else:\n",
    "                    # Error from above and below\n",
    "                    error_from_above = np.dot(self.weights[l].T, errors[l+1])\n",
    "                    if l > 0:  # For hidden layers, include lateral connections\n",
    "                        lateral_error = np.dot(self.lateral_weights[l-1], errors[l])\n",
    "                        errors[l] = error_from_above + lateral_error - (states[l] - predictions[l-1])\n",
    "                    else:  # For first hidden layer\n",
    "                        errors[l] = error_from_above - (states[l] - predictions[l-1])\n",
    "                \n",
    "                # Update state (gradient ascent on prediction accuracy)\n",
    "                state_update = self.learning_rate * errors[l]\n",
    "                if l > 0:  # Apply activation derivative for hidden layers\n",
    "                    state_update *= self.sigmoid_derivative(states[l])\n",
    "                states[l] += state_update\n",
    "            \n",
    "            # Recompute predictions with updated states\n",
    "            for l in range(len(self.weights)):\n",
    "                predictions[l] = self.sigmoid(np.dot(self.weights[l], states[l]))\n",
    "        \n",
    "        # Update weights (after inference iterations)\n",
    "        for l in range(len(self.weights)):\n",
    "            # Weight update is outer product of error and lower state\n",
    "            weight_update = np.outer(errors[l+1] * self.sigmoid_derivative(states[l+1]), states[l])\n",
    "            self.weights[l] += self.learning_rate * weight_update\n",
    "            \n",
    "            # Update lateral weights (except for input layer)\n",
    "            if l > 0:\n",
    "                lateral_update = np.outer(errors[l], errors[l])\n",
    "                self.lateral_weights[l-1] += 0.01 * self.learning_rate * lateral_update\n",
    "        \n",
    "        return predictions, errors\n",
    "    \n",
    "    def train(self, X, y, epochs=5, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the model on the given data.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data\n",
    "        - y: Target labels (one-hot encoded)\n",
    "        - epochs: Number of training epochs\n",
    "        - batch_size: Size of mini-batches\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            correct = 0\n",
    "            \n",
    "            # Shuffle data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_X = X_shuffled[i:i+batch_size]\n",
    "                batch_y = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                batch_loss = 0\n",
    "                batch_correct = 0\n",
    "                \n",
    "                for j in range(batch_X.shape[0]):\n",
    "                    # Perform predictive coding for this sample\n",
    "                    predictions, errors = self.update_states_and_weights(batch_X[j], batch_y[j])\n",
    "                    \n",
    "                    # Compute loss (squared prediction error at output layer)\n",
    "                    loss = 0.5 * np.sum(errors[-1]**2)\n",
    "                    batch_loss += loss\n",
    "                    \n",
    "                    # Count correct predictions\n",
    "                    predicted_class = np.argmax(predictions[-1])\n",
    "                    true_class = np.argmax(batch_y[j])\n",
    "                    if predicted_class == true_class:\n",
    "                        batch_correct += 1\n",
    "                \n",
    "                epoch_loss += batch_loss / batch_size\n",
    "                correct += batch_correct\n",
    "            \n",
    "            accuracy = correct / n_samples\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/n_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class probabilities\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            layer_predictions = self.forward_pass(x)\n",
    "            predictions.append(layer_predictions[-1])\n",
    "        return np.array(predictions)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Training the Model\n",
    "\n",
    "Now let's train our predictive coding model on the MNIST data.\n",
    "\"\"\"\n",
    "# %%\n",
    "# Initialize model\n",
    "pc_model = PredictiveCodingModel(layer_sizes=[784, 256, 10], \n",
    "                                learning_rate=0.01, \n",
    "                                n_iterations=5)\n",
    "\n",
    "# Train the model\n",
    "pc_model.train(X_train_prep, y_train_prep, epochs=10, batch_size=32)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Evaluating the Model\n",
    "\n",
    "Let's see how well our model performs on the test set.\n",
    "\"\"\"\n",
    "# %%\n",
    "# Make predictions on test set\n",
    "y_pred_probs = pc_model.predict(X_test_prep)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test_prep, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Visualizing Predictions\n",
    "\n",
    "Let's look at some example predictions.\n",
    "\"\"\"\n",
    "# %%\n",
    "# Plot some test samples and their predicted labels\n",
    "def plot_examples(X, y_true, y_pred, n_examples=10):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n_examples):\n",
    "        plt.subplot(1, n_examples, i+1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"True: {y_true[i]}\\nPred: {y_pred[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_examples(X_test, y_true, y_pred)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Understanding the Predictive Coding Process\n",
    "\n",
    "Let's examine what happens at each layer during prediction.\n",
    "\n",
    "### Layer-by-Layer Analysis\n",
    "1. **Input Layer (784 units)**: Receives pixel values\n",
    "2. **Hidden Layer (256 units)**: \n",
    "   - Receives predictions from output layer (top-down)\n",
    "   - Compares with bottom-up input from input layer\n",
    "   - Updates its state to minimize prediction error\n",
    "3. **Output Layer (10 units)**: \n",
    "   - Initially makes random predictions\n",
    "   - Updates based on comparison with true labels\n",
    "   - Sends corrected predictions back to hidden layer\n",
    "\n",
    "### Key Concepts:\n",
    "- **Predictions**: Each layer tries to predict the activity of the layer below\n",
    "- **Prediction Errors**: Differences between predictions and actual activity\n",
    "- **State Updates**: Neurons adjust their activity to minimize local prediction error\n",
    "- **Weight Updates**: Connections are adjusted to reduce future prediction errors\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Conclusion\n",
    "\n",
    "This implementation demonstrates a basic predictive coding model for MNIST classification. With more tuning (more iterations, larger network, more training data), the accuracy could be improved further. Predictive coding offers a biologically plausible alternative to backpropagation that performs local, iterative updates rather than global error propagation.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
