{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886cd651",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebebd753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:54:16.361847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750251256.399348   11715 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750251256.410875   11715 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750251256.439572   11715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750251256.439659   11715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750251256.439666   11715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750251256.439671   11715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-18 15:54:16.448290: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "def preprocess_data(X, y, n_samples=None):\n",
    "    \"\"\"Normalize images and convert labels to one-hot encoding\"\"\"\n",
    "    if n_samples is not None:\n",
    "        X = X[:n_samples]\n",
    "        y = y[:n_samples]\n",
    "    \n",
    "    # Flatten and normalize images (0-1 range)\n",
    "    X_flat = X.reshape(X.shape[0], -1).astype('float32')\n",
    "    X_norm = MinMaxScaler().fit_transform(X_flat)\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    y_onehot = np.zeros((len(y), 10))\n",
    "    y_onehot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return X_norm, y_onehot\n",
    "\n",
    "# Preprocess training and test data\n",
    "X_train_prep, y_train_prep = preprocess_data(X_train, y_train, n_samples=10000)\n",
    "X_test_prep, y_test_prep = preprocess_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960095bd",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "-  Loads MNIST dataset (28x28 grayscale digits 0-9)\n",
    "-  Normalizes pixel values to [0,1] range using MinMaxScaler\n",
    "\n",
    "- Converts labels to one-hot encoded vectors\n",
    "\n",
    "- Uses 10,000 samples for training (can be adjusted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c101fd",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc8d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictiveCodingModel:\n",
    "    def __init__(self, layer_sizes=[784, 256, 10], learning_rate=0.001, n_iterations=5):\n",
    "        \"\"\"Initialize model with layer sizes, learning rate, and iterations\"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        \n",
    "        # Initialize weights with He initialization (good for sigmoid/ReLU)\n",
    "        self.weights = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            std = np.sqrt(2. / layer_sizes[i])  # He initialization std\n",
    "            self.weights.append(np.random.normal(0, std, (layer_sizes[i+1], layer_sizes[i])))\n",
    "        \n",
    "        # Initialize small lateral connection weights\n",
    "        self.lateral_weights = []\n",
    "        for size in layer_sizes[1:]:\n",
    "            self.lateral_weights.append(0.01 * np.random.randn(size, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c802d94",
   "metadata": {},
   "source": [
    "**Layer Structure:**\n",
    "\n",
    "- Input Layer: 784 units (28x28 flattened image)\n",
    "\n",
    "- Hidden Layer: 256 units with lateral connections\n",
    "\n",
    "- Output Layer: 10 units (one per digit class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131ca83",
   "metadata": {},
   "source": [
    "# 3. Core Functions\n",
    "\n",
    "Sigmoid Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811f382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sigmoid(self, x):\n",
    "        \"\"\"Numerically stable sigmoid function\"\"\"\n",
    "        return np.where(x >= 0, \n",
    "                       1 / (1 + np.exp(-x)), \n",
    "                       np.exp(x) / (1 + np.exp(x)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid with clipping for stability\"\"\"\n",
    "        return np.clip(x * (1 - x), 1e-8, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb52b8",
   "metadata": {},
   "source": [
    "Forward Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6ff51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward_pass(self, x):\n",
    "        \"\"\"Compute predictions for each layer\"\"\"\n",
    "        layer_predictions = []\n",
    "        current_activity = x\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            # Compute prediction: σ(W·x)\n",
    "            prediction = self.sigmoid(np.dot(self.weights[i], current_activity))\n",
    "            layer_predictions.append(prediction)\n",
    "            current_activity = prediction\n",
    "        \n",
    "        return layer_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9e588",
   "metadata": {},
   "source": [
    "# 4. Predictive Coding Update Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee32727",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update_states_and_weights(self, x, y):\n",
    "        \"\"\"Core predictive coding update with lateral connections\"\"\"\n",
    "        # Initialize states (input + predictions)\n",
    "        states = [x.copy()]  # Input layer state\n",
    "        predictions = self.forward_pass(x)\n",
    "        states.extend(predictions)\n",
    "        states[-1] = y.copy()  # Replace top prediction with true label\n",
    "        \n",
    "        # Initialize errors for each layer\n",
    "        errors = [np.zeros_like(s) for s in states]\n",
    "        \n",
    "        # Iterative inference (n_iterations)\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Top-down error propagation\n",
    "            for l in range(len(self.weights), 0, -1):\n",
    "                # Compute prediction error\n",
    "                if l == len(self.weights):\n",
    "                    # Output layer error: target - prediction\n",
    "                    errors[l] = states[l] - predictions[l-1]\n",
    "                else:\n",
    "                    # Hidden layer error: top-down + lateral - prediction error\n",
    "                    error_from_above = np.dot(self.weights[l].T, errors[l+1])\n",
    "                    if l > 0:\n",
    "                        lateral_error = np.dot(self.lateral_weights[l-1], errors[l])\n",
    "                        errors[l] = error_from_above + 0.1 * lateral_error - (states[l] - predictions[l-1])\n",
    "                    else:\n",
    "                        errors[l] = error_from_above - (states[l] - predictions[l-1])\n",
    "                \n",
    "                # Update states with clipped gradients\n",
    "                state_update = 0.1 * self.learning_rate * np.clip(errors[l], -10, 10)\n",
    "                if l > 0:\n",
    "                    state_update *= self.sigmoid_derivative(states[l])\n",
    "                states[l] += state_update\n",
    "            \n",
    "            # Recompute predictions with updated states\n",
    "            for l in range(len(self.weights)):\n",
    "                predictions[l] = self.sigmoid(np.dot(self.weights[l], states[l]))\n",
    "        \n",
    "        # Weight updates\n",
    "        for l in range(len(self.weights)):\n",
    "            # Update forward weights: ΔW ∝ error * σ'(state) * input\n",
    "            weight_update = np.outer(\n",
    "                np.clip(errors[l+1], -5, 5) * self.sigmoid_derivative(states[l+1]), \n",
    "                states[l]\n",
    "            )\n",
    "            self.weights[l] += self.learning_rate * np.clip(weight_update, -0.1, 0.1)\n",
    "            \n",
    "            # Update lateral weights (if not input layer)\n",
    "            if l > 0:\n",
    "                lateral_update = np.outer(\n",
    "                    np.clip(errors[l], -5, 5), \n",
    "                    np.clip(errors[l], -5, 5)\n",
    "                )\n",
    "                self.lateral_weights[l-1] += 0.01 * self.learning_rate * lateral_update\n",
    "        \n",
    "        return predictions, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee642b",
   "metadata": {},
   "source": [
    "**Key Concepts:**\n",
    "\n",
    "- **Predictions:** Each layer's estimate of next layer's state\n",
    "\n",
    "- **Errors:** Difference between top-down predictions and actual states\n",
    "\n",
    "- **State Updates:** Adjustments to neuron activations based on errors\n",
    "\n",
    "- **Weight Updates:** Changes to connection strengths based on errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ab2e7",
   "metadata": {},
   "source": [
    "# 5. Training Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89848d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, X, y, epochs=5, batch_size=32):\n",
    "        \"\"\"Training loop with mini-batches\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data each epoch\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            correct = 0\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_X = X_shuffled[i:i+batch_size]\n",
    "                batch_y = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                batch_loss = 0\n",
    "                batch_correct = 0\n",
    "                \n",
    "                for j in range(batch_X.shape[0]):\n",
    "                    # Process each sample in batch\n",
    "                    predictions, errors = self.update_states_and_weights(batch_X[j], batch_y[j])\n",
    "                    \n",
    "                    # Compute regularized loss\n",
    "                    reg_loss = 0\n",
    "                    for w in self.weights:\n",
    "                        reg_loss += 0.001 * np.sum(w**2)  # L2 regularization\n",
    "                    \n",
    "                    loss = 0.5 * np.sum(errors[-1]**2) + reg_loss\n",
    "                    batch_loss += loss\n",
    "                    \n",
    "                    # Track accuracy\n",
    "                    predicted_class = np.argmax(predictions[-1])\n",
    "                    true_class = np.argmax(batch_y[j])\n",
    "                    if predicted_class == true_class:\n",
    "                        batch_correct += 1\n",
    "                \n",
    "                epoch_loss += batch_loss / batch_size\n",
    "                correct += batch_correct\n",
    "            \n",
    "            # Epoch statistics\n",
    "            accuracy = correct / n_samples\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/n_samples:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9fb0f",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d5dd48",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3392337019.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef predict(self, X):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "    def predict(self, X):\n",
    "        \"\"\"Generate predictions for input data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            layer_predictions = self.forward_pass(x)\n",
    "            predictions.append(layer_predictions[-1])  # Return output layer\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Initialize and train model\n",
    "pc_model = PredictiveCodingModel(layer_sizes=[784, 256, 10], \n",
    "                                learning_rate=0.001,\n",
    "                                n_iterations=5)\n",
    "\n",
    "# Train for 20 epochs (can reduce to 10 if needed)\n",
    "pc_model.train(X_train_prep, y_train_prep, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_probs = pc_model.predict(X_test_prep)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test_prep, axis=1)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
